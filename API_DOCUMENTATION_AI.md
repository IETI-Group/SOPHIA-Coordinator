# Documentaci√≥n de la API del Servicio de IA

Esta documentaci√≥n detalla c√≥mo configurar, ejecutar y utilizar los servicios de Inteligencia Artificial integrados en SOPHIA Coordinator.

---

## üì° Endpoints de la API

URL Base: `/api/v1/ai`

### 1. Chat con IA

Interact√∫a con el asistente de IA. Soporta historial de conversaci√≥n para respuestas conscientes del contexto (memoria).

**Endpoint:** `POST /chat`

#### Cuerpo de la Petici√≥n (Request Body)

| Campo | Tipo | Requerido | Descripci√≥n |
|-------|------|----------|-------------|
| `message` | string | S√≠ | El mensaje actual que se env√≠a a la IA. |
| `history` | array | No | Lista de mensajes previos para dar contexto. |

**Estructura del Objeto Historial:**
```json
{
  "role": "user" | "assistant" | "system",
  "content": "string"
}
```

#### ¬øQu√© es el `role`?
El campo `role` indica qui√©n dijo el mensaje en el historial, permitiendo a la IA entender el flujo de la conversaci√≥n:
*   **`user`**: Eres t√∫ (el usuario). Representa las preguntas o entradas que has hecho anteriormente.
*   **`assistant`**: Es la IA. Representa las respuestas que la IA te ha dado antes.
*   **`system`**: (Opcional) Define el comportamiento general o personalidad de la IA.

#### Ejemplo de Petici√≥n

**Mensaje Simple (Sin memoria):**
```json
{
  "message": "¬øCu√°l es la capital de Francia?"
}
```

**Mensaje con Historial (Con memoria):**
```json
{
  "message": "Cu√©ntame m√°s sobre su cultura.",
  "history": [
    {
      "role": "user",
      "content": "¬øCu√°l es la capital de Francia?"
    },
    {
      "role": "assistant",
      "content": "La capital de Francia es Par√≠s."
    }
  ]
}
```
*Nota: Al enviar el historial, la IA sabe que "su cultura" se refiere a la cultura de Par√≠s.*

#### Respuesta Exitosa

**C√≥digo:** `200 OK`

```json
{
  "response": "La cultura parisina es famosa por su arte, moda, gastronom√≠a y arquitectura..."
}
```

#### Respuesta de Error

**C√≥digo:** `400 Bad Request`
```json
{
  "success": false,
  "error": "Missing required field: message",
  "timestamp": "2023-10-27T10:00:00.000Z"
}
```

---

### 2. Asistente de Cursos

Genera un esquema de curso estructurado y profesional basado en una idea y pautas de estilo.

**Endpoint:** `POST /course-assistant`

#### Cuerpo de la Petici√≥n

| Campo | Tipo | Requerido | Descripci√≥n |
|-------|------|----------|-------------|
| `idea` | string | S√≠ | El concepto central o tema del curso. |
| `guide` | string | S√≠ | Pautas estructurales, audiencia objetivo o requisitos espec√≠ficos. |

#### Ejemplo de Petici√≥n

```json
{
  "idea": "Introducci√≥n a la Programaci√≥n en Python para Ciencia de Datos",
  "guide": "La audiencia objetivo son principiantes. Incluir 4 m√≥dulos principales. Enfocarse en ejemplos pr√°cticos."
}
```

#### Respuesta Exitosa

**C√≥digo:** `200 OK`

```json
{
  "response": "T√≠tulo del Curso: Python para Ciencia de Datos\n\nM√≥dulo 1: Fundamentos de Python\n- Lecci√≥n 1.1: Instalaci√≥n y Configuraci√≥n\n- Lecci√≥n 1.2: Variables y Tipos de Datos..."
}
```

#### Respuesta de Error

**C√≥digo:** `400 Bad Request`
```json
{
  "success": false,
  "error": "Missing required field: idea",
  "timestamp": "2023-10-27T10:00:00.000Z"
}
```


## üõ†Ô∏è Prerrequisitos y Configuraci√≥n

Para que el servicio de IA funcione correctamente, es necesario tener **Ollama** instalado y ejecut√°ndose localmente (o en un servidor accesible).

### 1. Instalar Ollama
Ollama es la herramienta que nos permite ejecutar modelos de lenguaje (LLMs) localmente.
- **Descargar:** Visita [ollama.com](https://ollama.com) y descarga la versi√≥n para tu sistema operativo.
- **Instalar:** Sigue las instrucciones del instalador.

### 2. Descargar el Modelo
El proyecto est√° configurado para usar un modelo espec√≠fico (definido en el archivo `.env`). Debes asegurarte de tener ese modelo descargado en Ollama.

Por defecto, si tu `.env` dice `OLLAMA_MODEL=llama3.2`, ejecuta en tu terminal:
```bash
ollama pull llama3.2
```
*Nota: Verifica la variable `OLLAMA_MODEL` en tu archivo `.env` para saber qu√© modelo descargar.*

### 3. Configuraci√≥n de Variables de Entorno (.env)
Aseg√∫rate de que tu archivo `.env` tenga las siguientes variables configuradas correctamente:

```dotenv
# Configuraci√≥n de IA
OLLAMA_HOST=http://127.0.0.1:11434  # URL donde corre Ollama (por defecto es esta)
OLLAMA_MODEL=llama3.2               # El modelo a utilizar (ej: llama3.2, llama2, mistral)
```

### 4. Verificar que Ollama est√° corriendo
Antes de iniciar el servidor, aseg√∫rate de que Ollama est√© activo. Puedes verificarlo entrando a `http://127.0.0.1:11434` en tu navegador (deber√≠a decir "Ollama is running").

### 5. verificar que este corriendo el servicio ejecutando
```bash
pnpm dev
```

